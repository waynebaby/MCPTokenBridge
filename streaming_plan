# Streaming Plan (MCPTokenBridge)

简要结论（Feasibility）
- 可以实现：在现有「MCP hook 循环 + FastAPI」架构中，引入两个队列：请求任务队列和按 `requestID` 复用/隔离的数据队列，支持边产出边推送。
- 注意事项：当前项目文档明确“全局禁用 streaming”；建议以环境变量开关（默认关闭）实现可选的 streaming，不影响现有非流式行为与测试。
- 风险与缓解：客户端断开、MCP无流式回调、背压/超时、清理与泄漏。需在队列管理与HTTP生成器中显式处理。

目标（Behavior）
- 当 HTTP 请求携带 `stream=true`：
    1) HTTP 为该请求创建独立的输出队列 `out_queue`，并将 `(out_queue, req)` 放入共享的「请求队列」。
    2) HTTP 立刻开始从 `out_queue` 读取片段消息 `{"msgFrag":"..."}` 并以 SSE/分块形式转发。
    3) MCP hook 在采样过程中，不断向 `out_queue` 推送 token/文本片段。
    4) 结束哨兵：`{"msgFrag":null}`；HTTP 读取到该消息后终止响应。
- 当 `stream=false` 或 streaming 开关关闭：
    1) 仍创建独立 `out_queue` 并入队 `(out_queue, req)`。
    2) MCP hook 生成完整文本后，向队列推送两条消息：一条全文 `{"msgFrag":"<full>"}`，一条结束哨兵 `{"msgFrag":null}`。
    3) HTTP 读取第一条即得到全文；读取到第二条后结束并返回一次性 JSON。

核心设计（Design）
1. 类型与数据结构（Types & Data Structures）
    - `StreamMsg`: dataclass，字段：`msgFrag: Optional[str]`（`None` 表示结束）。
    - `request_queue: asyncio.Queue[tuple[asyncio.Queue[StreamMsg], ChatRequest]]`：HTTP 为每个请求创建独立的 `asyncio.Queue`，与请求一起入队。
    - 每请求独立队列（per-request queue）：不再需要 `requestID` 关联；HTTP 与 hook 通过队列实例直接对接。

2. MCP hook 集成（Hook Integration）
     - Hook 循环从 `request_queue` 读取 `(out_queue, req)`；当检测到任务 `stream=true`：
         - 开始采样；若 MCP/ctx 支持原生流式回调，则在每次输出时向 `out_queue` 推送 `StreamMsg(msgFrag=...)`。
         - 若 MCP 不支持流式，仅返回整段文本，则在 hook 中进行片段化（例如按 token/句子/固定长度），逐段推送；最后推送结束哨兵 `StreamMsg(msgFrag=None)`。
     - 任一错误（取消、超时、ctx错误）均确保发送结束哨兵，避免 HTTP 侧阻塞；队列对象仅由该请求持有，完成后由 GC 回收。

3. HTTP 层（FastAPI）
     - 保持现有非流式路由；当 `stream=true` 且开启 streaming：
         - 为该请求创建独立 `out_queue = asyncio.Queue[StreamMsg]`。
         - 将 `(out_queue, req)` 放入 `request_queue`。
         - 返回 `StreamingResponse`，`media_type='text/event-stream'`（SSE）或文本分块。
         - 生成器逻辑：
             - `while True: msg = await out_queue.get()`；若 `msg.msgFrag is None` -> 终止；否则发送片段事件。
             - SSE 事件体可直接发送 `{ "msgFrag": "..." }`，或按 OpenAI/Anthropic 兼容格式封装为 `delta`。
         - 断开/取消：捕获 `ClientDisconnect`，可选向 hook 发出取消信号（例如通过共享的 `asyncio.Event` 或在请求体中携带取消令牌）。

4. 协议与格式（Protocol & Format）
    - 数据队列消息格式：`{"msgFrag": "<text>"}`；结束为 `{"msgFrag": null}`。
    - 若需兼容 OpenAI/Anthropic SSE 格式，可在 HTTP 侧将 `msgFrag` 包装成对应事件类型（例如 OpenAI 的 `delta.content`）。
    - 为便于调试，内部事件保持简化 JSON，外层按需转换。

5. 环境与开关（Env & Gating）
   - 新增 `MCPTB_ENABLE_STREAMING`（默认关闭）。关闭时忽略 `stream=true`（维持非流式）。
   - 继续保留现有就绪/503 逻辑；未就绪时拒绝流式与非流式。
   - 可选：`MCPTB_STREAM_TIMEOUT` 控制 HTTP 侧最长等待时间（订阅生成器级别）。

6. 清理与健壮性（Cleanup & Robustness）
   - 无论正常结束或异常，始终发送结束哨兵并清理 `stream_queues[req_id]`。
   - 背压：若 HTTP 端处理较慢，队列尺寸可设上限并在 hook 侧 `await put`；必要时丢弃/合并片段以保证实时性。
   - 取消：若客户端断开，HTTP 生成器退出，调用 `close(req_id)`；hook 端可根据 `req_id` 标记取消采样（需要在 ctx.sample 层支持或模拟）。

7. 实现步骤（Implementation Steps）
   1) 新增类型：`StreamMsg`、`StreamQueueManager`，以及 `stream_queues` 容器；为请求生成稳定的 `req_id`（UUID）。
   2) HTTP 路由：识别 `stream=true`，分配 `req_id`；将完整请求（含 `req_id`）入 `request_queue`；返回 `StreamingResponse`，使用异步生成器订阅 `req_id` 队列。
   3) MCP hook：拉取请求并执行 `ctx.sample`；每次片段产出时 `emit(req_id, frag)`；结束时 `emit(req_id, None)`；异常路径也必须发送 `None` 并 `close(req_id)`。
   4) 兼容格式：如需 OpenAI/Anthropic SSE 兼容，在 HTTP 生成器内将 `msgFrag` 转译为对应事件；否则直接传内部 JSON。
   5) 开关与超时：增加 `MCPTB_ENABLE_STREAMING` 与可选 `MCPTB_STREAM_TIMEOUT`；默认关闭确保现状不变。
   6) 端到端测试：
      - 正常流：多个片段 + 结束哨兵，客户端逐步接收。
      - 超时/取消：客户端中途断开，队列清理无泄漏；hook 收到取消（若支持）。
      - 并发：多 `req_id` 并行，互不干扰，吞吐合理。

8. 伪代码（Pseudo-code）
```python
@dataclass
class StreamMsg:
    msgFrag: Optional[str]

# FastAPI
@app.post('/v1/chat/completions')
async def chat(req: ChatCompletionRequest):
    if not ENABLE_STREAMING or not req.stream:
        # existing non-stream path
        return await non_stream_handler(req)
    out_queue: asyncio.Queue[StreamMsg] = asyncio.Queue(maxsize=1024)
    await request_queue.put((out_queue, req))
    async def sse_gen():
        while True:
            m = await out_queue.get()
            if m.msgFrag is None:
                break
            yield f"data: {json.dumps({'msgFrag': m.msgFrag})}\n\n"
    return StreamingResponse(sse_gen(), media_type='text/event-stream')

# MCP hook loop (simplified)
async def hook_loop(ctx):
    while True:
        out_queue, req = await request_queue.get()
        try:
            async for frag in ctx.sample_stream(req):  # if supported
                await out_queue.put(StreamMsg(frag))
        finally:
            # 必须：采样结束后推送终止哨兵（null）/ Always send final sentinel
            await out_queue.put(StreamMsg(None))
```

11. 并发模型（Concurrency Model）
- 目标：支持多线程读写（HTTP 在主 asyncio 事件循环，MCP hook 可能位于另一线程），同时保持最小依赖。
- 策略：统一使用主事件循环中的 `asyncio.Queue`；跨线程写入通过 `loop.call_soon_threadsafe(...)`；跨线程读取通过 `asyncio.run_coroutine_threadsafe(..., loop)`。

推荐做法（不引入外部依赖）：
```python
class StreamEmitter:
    """跨线程安全的片段推送器。Thread-safe emitter scheduling puts onto the main asyncio loop."""
    def __init__(self, loop: asyncio.AbstractEventLoop, queue: asyncio.Queue):
        self._loop = loop
        self._queue = queue
    def emit(self, frag: Optional[str]):
        # 调度到事件循环，避免直接跨线程操作 asyncio.Queue
        self._loop.call_soon_threadsafe(self._queue.put_nowait, StreamMsg(frag))

# 全局：主事件循环中的队列
loop = asyncio.get_running_loop()
request_queue: asyncio.Queue[tuple[asyncio.Queue[StreamMsg], ChatCompletionRequest]] = asyncio.Queue()

@app.post('/v1/chat/completions')
async def chat(req: ChatCompletionRequest):
    if not ENABLE_STREAMING or not req.stream:
        return await non_stream_handler(req)
    out_queue: asyncio.Queue[StreamMsg] = asyncio.Queue(maxsize=1024)
    await request_queue.put((out_queue, req))
    async def sse_gen():
        while True:
            m = await out_queue.get()
            if m.msgFrag is None:
                break
            yield f"data: {json.dumps({'msgFrag': m.msgFrag})}\n\n"
    return StreamingResponse(sse_gen(), media_type='text/event-stream')

# Hook 线程中：从 request_queue 读取任务（跨线程）
def hook_thread_main(ctx, loop: asyncio.AbstractEventLoop):
    while True:
        fut = asyncio.run_coroutine_threadsafe(request_queue.get(), loop)
        out_queue, req = fut.result()
        emitter = StreamEmitter(loop, out_queue)
        try:
            for frag in sample_fragments(ctx, req):  # 同步或异步片段生成
                emitter.emit(frag)
        finally:
            # 必须：采样结束后推送终止哨兵（null）/ Always send final sentinel
            emitter.emit(None)
```

说明：
- 每个请求独立队列（per-request queue）是推荐的：隔离性好、易于清理和背压控制；不再需要 `requestID` 过滤与映射。
- 将 `asyncio.Queue` 作为对象随请求传递，但跨线程写入仍需通过 `loop.call_soon_threadsafe`（见 `StreamEmitter`），避免直接操作队列。
- 若未来需要更简单的跨线程桥，可考虑引入 `janus`（双向桥接 `asyncio.Queue` 与 `queue.Queue`），但目前方案无额外依赖即可满足。

9. 对现有行为的影响（Impact）
- 默认关闭 streaming，不改变当前非流式接口与测试。开启时新增 SSE 行为，仅在 `stream=true` 时触发。
- MCP hook 的主路径不变，仅在检测到流式请求时额外逐片推送到数据队列。

10. 后续（Next）
- 若您确认上述方案，下一步我将：
    - 在 `mcptb.py` 中最小化引入 `StreamMsg`，并实现 per-request 队列桥接（`request_queue` 传递 `(out_queue, req)`）。
  - 为 `/v1/chat/completions` 和 `/v1/messages` 添加 `StreamingResponse` 分支（受环境变量控制）。
  - 在 hook 工具中提供片段推送（若无原生流式，则先用简单分段模拟）。
  - 增加基础端到端流式测试与断开/清理测试。

12. 统一非流式路径（Unified Non-Streaming over Queues）
- 动机：即便 `stream=false`，也复用相同的 per-request 队列机制；这样统一生命周期管理、背压与取消处理，减少分支复杂度。
- 行为：非流式请求依然分配 `req_id` 和独立队列，HTTP 端消费队列，收集片段直至读到结束哨兵（`msgFrag: None`），然后拼接为最终字符串并返回标准 JSON。
- 优点：
    - 统一数据通道：hook/HTTP 两端只保留一种产出/消费模型。
    - 易于调试：可以在非流式下观察片段（例如按固定长度分片），定位问题。
    - 可切换：未来若按需开启流式，仅需在 HTTP 层切换为 SSE 输出，不改 hook。
- 注意：
    - 性能：相比一次性返回，多一次队列往返；但好处是语义统一，且队列容量可控。
    - 必须确保在任意路径（正常/异常）发送结束哨兵，避免 HTTP 端阻塞。

非流式伪代码：
```python
@app.post('/v1/chat/completions')
async def chat(req: ChatCompletionRequest):
    out_queue: asyncio.Queue[StreamMsg] = asyncio.Queue(maxsize=1024)
    await request_queue.put((out_queue, req))
    if ENABLE_STREAMING and req.stream:
        async def sse_gen():
            while True:
                m = await out_queue.get()
                if m.msgFrag is None:
                    break
                yield f"data: {json.dumps({'msgFrag': m.msgFrag})}\n\n"
        return StreamingResponse(sse_gen(), media_type='text/event-stream')
    else:
        # 读取一条全文，再读取终止哨兵
        first = await out_queue.get()
        _end = await out_queue.get()  # expect msgFrag is None
        text = first.msgFrag or ""
        return ChatCompletionResponse(choices=[Choice(message=ChatMessage(role='assistant', content=text))])
```
